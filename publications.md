---
layout: default
title: Publications/Conference Papers
permalink: /publication/
---

## A Causal Model to Quantify Edge and Cumulative Unfairness for Unfair Edge Prioritization and Discrimination Removal
##### [P. Ravishankar](https://sites.google.com/view/pavanravishankar/home), [P. Malviya](https://in.linkedin.com/in/pranshumalviya2), [B. Ravindran](http://www.cse.iitm.ac.in/~ravi/)
#### Under Review 
##### 2021
###### Submitted to [the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)](https://facctconference.org)
--------------------------------------------------

## [A Causal Linear Model to Quantify Edge and Cumulative Unfairness for Unfair Edge Prioritization and Discrimination Removal](https://arxiv.org/abs/2007.05516)
##### [P. Ravishankar](https://sites.google.com/view/pavanravishankar/home), [P. Malviya](https://in.linkedin.com/in/pranshumalviya2), [B. Ravindran](http://www.cse.iitm.ac.in/~ravi/)
#### [Workshop on Law and Machine Learning](https://sites.google.com/view/icml-law-and-ml-2020/), [ICML](https://icml.cc)
##### 2020
###### Abstract: The dataset can be generated by an unfair mechanism in numerous settings. For instance, a judicial system is unfair if it rejects the bail plea of an accused based on the race. To mitigate the unfairness in the procedure generating the dataset, we need to identify the sources of unfairness, quantify the unfairness in these sources, quantify how these sources affect the overall unfairness, and prioritize the sources before addressing the real-world issues underlying them. Prior work of Zhang et al. 2017 identifies and removes discrimination after data is generated but does not suggest a methodology to mitigate unfairness in the data generation phase. We use the notion of an unfair edge, same as Chiappa et al. 2018, to be a source of discrimination and quantify unfairness along an unfair edge. We also quantify overall unfairness in a particular decision towards a subset of sensitive attributes in terms of edge unfairness and measure the sensitivity of the former when the latter is varied. Using the formulation of cumulative unfairness in terms of edge unfairness, we alter the discrimination removal methodology discussed in Zhang et al. 2017 by not formulating it as an optimization problem. This helps in getting rid of constraints that grow exponentially in the number of sensitive attributes and values taken by them. Finally, we discuss a priority algorithm for policymakers to address the real-world issues underlying the edges that result in unfairness. The experimental section validates the linear model assumption made to quantify edge unfairness. 

--------------------------------------------------

## [Contextual Care Protocol using Neural Networks and Decision Trees](https://arxiv.org/pdf/1811.06437)
##### [Y. P. Sinha](https://in.linkedin.com/in/yash-pratyush-sinha-0b4756ba), [P. Malviya](https://in.linkedin.com/in/pranshumalviya2), [M. Panda](https://in.linkedin.com/in/minerva-panda-22b04593), [S. M. Ali](https://www.linkedin.com/in/syedmohdali121/)
#### [Second International Conference on Advances in Electronics, Computer and Communications (IEEE Xplore)](http://reva.edu.in/icaecc2018/)
##### 2018
###### Abstract: A contextual care protocol is used by a medical practitioner for patient healthcare, given the context or situation that the specified patient is in. This paper proposes a method to build an automated self-adapting protocol which can help make relevant, early decisions for effective healthcare delivery. The hybrid model leverages neural networks and decision trees. The neural network estimates the chances of each disease and each tree in the decision trees represents care protocol for a disease. These trees are subject to change in case of aberrations found by the diagnosticians. These corrections or prediction errors are clustered into similar groups for scalability and review by the experts. The corrections as suggested by the experts are incorporated into the model.
--------------------------------------------------
